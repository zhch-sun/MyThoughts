王树森 ISBN 9787115600691
# 多智能体系统
多智能体系统MAS, multi-agent system. 单智能体系统single-agent system(SAS). 
多智能体强化学习MARL, multi-agent reinforcement learning. 
四种常见设定:
合作, 同一目标, 获得奖励相同. 
竞争, 零和博弈. 
混合, 智能体分成多个群组, 组内是合作, 组间是竞争. 机器人足球. 
利己主义, 互相影响, 但不是零和博弈. 
实验环境: 
multi-agent particle world
1. 合作环境, 合作导航: 均为纯合作情况. 
2. 捕食者, 猎物: 多个捕食者, 和一个猎物. 捕食者之间合作, 捕食者和猎物竞争. 
StarCraft multi-agent challenge: 星际争霸
Hanabi Challenge: 合作型卡牌游戏, 
## 完全合作的MARL
智能体对全局状态只有一个局部观测. 
完全合作关系: 所有智能体具有相同的奖励/回报.  有相同的价值函数. `策略网络一致? `
难点: 信息不完全
解法: 1. 交换信息. 2.对策略网络和价值函数做近似. 

三种常见架构
1. 完全中心化: 训练决策都需要通信. 
2. 完全去中心化: 对网络做近似, 避免通信
3. 中心化训练, 去中心化决策: 训练中通信, 决策时不要. 
## 非合作关系设定下的多智能体强化学习
每个智能体的目标函数, 依赖于其他智能体的策略. 
收敛判别: 非合作关系下, 收敛标准是纳什均衡. 
三种架构: 和合作关系一样, 都是对通信的处理. 
